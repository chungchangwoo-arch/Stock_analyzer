import streamlit as st
import requests
from groq import Groq
import pandas as pd
import FinanceDataReader as fdr
import plotly.graph_objects as go
import plotly.express as px 
from plotly.subplots import make_subplots
import re
import html
import json
import time
from datetime import datetime, timedelta, date
from dateutil.relativedelta import relativedelta
import numpy as np

# --------------------------------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì •
# --------------------------------------------------------------------------
st.set_page_config(page_title="AlphaView: Deep Dive", page_icon="âš¡", layout="wide")

try:
    NAVER_ID = st.secrets["naver"]["client_id"]
    NAVER_SECRET = st.secrets["naver"]["client_secret"]
    GROQ_KEY = st.secrets["groq"]["api_key"]
except:
    st.error("ğŸš¨ secrets.toml ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()

client = Groq(api_key=GROQ_KEY)

# --------------------------------------------------------------------------
# 2. ìœ í‹¸ë¦¬í‹° & ë°ì´í„° ìˆ˜ì§‘
# --------------------------------------------------------------------------

@st.cache_data(ttl=86400)
def get_krx_code_map():
    try:
        df = fdr.StockListing('KRX')
        return dict(zip(df['Name'], df['Code']))
    except:
        return {}

def find_ticker(name, code_map):
    if name in code_map: return code_map[name]
    if name.isdigit() and len(name) == 6: return name
    return name.upper()

def clean_text(text):
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    return text.strip()

def summarize_title(text, max_length=60):
    """
    ê¸´ ê¸°ì‚¬ ì œëª©ì„ ê°„ë‹¨íˆ ìš”ì•½
    - ê´„í˜¸ ì œê±° (ê¸°ì‚¬ ì¶œì²˜ ë“±)
    - ê¸¸ì´ ì´ˆê³¼ ì‹œ í•µì‹¬ë§Œ ì¶”ì¶œ
    """
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì˜ˆ: [ê¸°ì‚¬ì¶œì²˜], (ë¶„ì„), ë“±)
    text = re.sub(r'[\(\[].*?[\)\]]', '', text).strip()
    
    # ê¸¸ì´ê°€ ê¸¸ë©´ '-', '|' ë“±ìœ¼ë¡œ ì²« ë²ˆì§¸ ì ˆë§Œ ì¶”ì¶œ
    if len(text) > max_length:
        for delimiter in [' - ', ' | ', ' / ', '...']:
            if delimiter in text:
                text = text.split(delimiter)[0].strip()
                break
    
    # ì—¬ì „íˆ ê¸¸ë©´ max_lengthë¡œ ìë¥´ê³  '...' ì¶”ê°€
    if len(text) > max_length:
        text = text[:max_length] + '...'
    
    return text

@st.cache_data(ttl=600)
def get_stock_data(ticker, start_date, end_date):
    try:
        df = fdr.DataReader(ticker, start_date, end_date)
        return df
    except:
        return None

@st.cache_data(ttl=3600)
def get_naver_datalab_trend(keyword, start_date, end_date):
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": NAVER_ID,
        "X-Naver-Client-Secret": NAVER_SECRET,
        "Content-Type": "application/json"
    }
    body = {
        "startDate": start_date.strftime("%Y-%m-%d"),
        "endDate": end_date.strftime("%Y-%m-%d"),
        "timeUnit": "date",
        "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
    }
    try:
        response = requests.post(url, headers=headers, data=json.dumps(body))
        if response.status_code == 200:
            result = response.json()
            if not result['results']: return pd.DataFrame()
            data_list = result['results'][0]['data']
            df_trend = pd.DataFrame(data_list)
            df_trend['period'] = pd.to_datetime(df_trend['period'])
            df_trend.set_index('period', inplace=True)
            df_trend.rename(columns={'ratio': 'search_volume'}, inplace=True)
            return df_trend
        else: return pd.DataFrame()
    except: return pd.DataFrame()

@st.cache_data(ttl=600)
def get_naver_news_content(keyword, start_date, end_date):
    """
    ë‰´ìŠ¤ ê²€ìƒ‰ (keyword ì •í™•ë„ í•„í„°ë§Œ ì ìš©)
    ê³¼ê±° ë°ì´í„°ì˜ ë‰´ìŠ¤ ë¶€ì¡± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ì¬ë¬´ í‚¤ì›Œë“œ í•„í„° ì œê±°
    """
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    all_items = []
    curr = start_date.replace(day=1)
    end = end_date.replace(day=1)
    
    # ì •ê·œì‹: ì •í™•í•œ ë‹¨ì–´ ê²½ê³„ë¡œ keyword í¬í•¨ í•„í„°
    import re as re_module
    keyword_pattern = re_module.compile(rf'\b{re_module.escape(keyword)}\b')
    
    while curr <= end:
        year_month = curr.strftime("%Yë…„ %mì›”")
        query = f"{keyword} {year_month}"
        # display 100ìœ¼ë¡œ ì¦ê°€, sort='sim'(ìœ ì‚¬ë„)ë¡œ ë³€ê²½í•´ ê´€ë ¨ì„± ë†’ì€ ê²ƒ ìš°ì„ 
        params = {"query": query, "display": 100, "sort": "sim"}
        try:
            res = requests.get(url, headers=headers, params=params, timeout=5)
            if res.status_code == 200:
                items = res.json().get('items', [])
                for item in items:
                    try:
                        pub_date = datetime.strptime(item['pubDate'], "%a, %d %b %Y %H:%M:%S %z").replace(tzinfo=None)
                        if start_date <= pub_date.date() <= end_date:
                            title = clean_text(item.get('title', ''))
                            # í•„í„°: ì •í™•í•œ keyword í¬í•¨ ì—¬ë¶€ë§Œ ì²´í¬
                            if not keyword_pattern.search(title):
                                continue
                            item['clean_date'] = pub_date
                            item['date_str'] = pub_date.strftime("%Y-%m-%d")
                            item['summary_title'] = summarize_title(title)  # ìš”ì•½ëœ ì œëª©ë„ ì €ì¥
                            all_items.append(item)
                    except: continue
        except: pass
        curr += relativedelta(months=1)
        time.sleep(0.1)
    
    all_items.sort(key=lambda x: x['clean_date'])
    # ì¤‘ë³µì œê±°
    unique = []
    seen = set()
    for i in all_items:
        if i['title'] not in seen:
            unique.append(i)
            seen.add(i['title'])
    return unique

# --------------------------------------------------------------------------
# 3. AI ë¶„ì„ í•¨ìˆ˜ë“¤ (ì¼ë°˜ ë¶„ì„ + [NEW] ê¸‰ë“±ë½ ë¶„ì„)
# --------------------------------------------------------------------------

def analyze_general_trend(keyword, trend_df, news_items):
    """ì „ì²´ ê¸°ê°„ íŠ¸ë Œë“œ ë¶„ì„"""
    max_date = trend_df['search_volume'].idxmax().strftime("%Y-%m-%d") if not trend_df.empty else "N/A"
    step = max(1, len(news_items) // 15)
    context = "\n".join([f"- [{i['date_str']}] {clean_text(i['title'])}" for i in news_items[::step][:15]])

    prompt = f"""
    Target: {keyword}, Peak Date: {max_date}
    News: {context}
    Output JSON: {{ "summary": "Period summary", "peak_reason": "Reason for peak interest", "sentiment": "Sentiment" }}
    """
    try:
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role":"system","content":"Analyze financial trend. JSON only."},{"role":"user","content":prompt}],
            temperature=0.1, response_format={"type": "json_object"}
        )
        return json.loads(completion.choices[0].message.content)
    except: return None

def analyze_top_volatility(keyword, events_df, news_items):
    """
    [NEW] ì´ë²¤íŠ¸(ê¸°ê°„) ë‹¨ìœ„ë¡œ ê¸‰ë“±ë½ì˜ ì›ì¸ì„ ë¶„ì„ (ë‰´ìŠ¤ ê¸°ë°˜)
    events_df: DataFrame with columns ['Start','End','PeakDate','PeakChange','PeakChangeAbs','Trigger','NewsList']
    ë‰´ìŠ¤ê°€ ì¶©ë¶„í•œ ì´ë²¤íŠ¸ë§Œ ë¶„ì„
    """
    if events_df is None or events_df.empty:
        return None

    # ë‰´ìŠ¤ê°€ ìµœì†Œ 2ê°œ ì´ìƒì¸ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
    events_with_news = events_df[events_df['NewsList'].apply(lambda x: len(x) >= 2 if isinstance(x, list) else False)]
    
    if events_with_news.empty:
        # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ë¶„ì„ ë¶ˆê°€
        return {"events": [], "note": "ë‰´ìŠ¤ ë°ì´í„° ë¶€ì¡±"}

    events_context = ""
    for _, ev in events_with_news.iterrows():
        start = ev['Start'].strftime("%Y-%m-%d")
        end = ev['End'].strftime("%Y-%m-%d")
        peak = ev['PeakDate'].strftime("%Y-%m-%d")
        change = ev['PeakChange']
        trigger = ev.get('Trigger', '')

        # ë‰´ìŠ¤ í¬í•¨ (ì´ë¯¸ í•„í„°ë§ë˜ì–´ ìˆìŒ)
        related_news = ev.get('NewsList', [])
        if related_news:
            news_str = " | ".join([f"[{n.get('date', '')}] {n.get('title', '')}" for n in related_news[:5]])
        else:
            news_str = "(ë‰´ìŠ¤ ì—†ìŒ)"
        
        events_context += f"ã€ì´ë²¤íŠ¸ã€‘\nê¸°ê°„: {start} ~ {end}\ní”¼í¬: {peak}\në³€ë™í­: {change:.2f}%\në‰´ìŠ¤: {news_str}\n\n"

    prompt = f"""
    ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì£¼ì‹ ê¸‰ë“±ë½ ì´ë²¤íŠ¸ë“¤ì„ ë‰´ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”. 
    ë‰´ìŠ¤ì—ì„œ ëª…í™•í•œ ì›ì¸ì„ ì°¾ì•„ ê·¸ê²ƒì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ë‰´ìŠ¤ì™€ ì§ì ‘ì ì¸ ì—°ê´€ì´ ì—†ëŠ” ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”.

    ë¶„ì„ ê°€ì´ë“œ:
    1. ì£¼ì–´ì§„ ë‰´ìŠ¤ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë¶„ì„
    2. ë‰´ìŠ¤ì— ëª…ì‹œëœ ì‚¬ê±´/ê³µì‹œ/ê²°ê³¼ ë“±ì„ ì§ì ‘ ì¸ìš©
    3. ë‰´ìŠ¤ ì—†ì´ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ ê²ƒ
    4. ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° "ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„ ë¶ˆê°€ëŠ¥" ëª…ì‹œ
    5. í•´ë‹¹ ê¸°ê°„ì˜ ì£¼ê°€ ì›€ì§ì„ì— ëŒ€í•œ ì •ì„±ì  í‰ê°€ ì¶”ê°€ (í˜¸ì¬/ì•…ì¬, ê¸°ëŒ€ê°, ì¡°ì • ë“±)

    ì´ë²¤íŠ¸ ë°ì´í„° (ê¸°ê°„, í”¼í¬ ë³€ë™í­, ê´€ë ¨ ë‰´ìŠ¤):
    {events_context}

    ì‘ë‹µ í˜•ì‹ (JSONìœ¼ë¡œë§Œ ì‘ë‹µ):
    {{
      "events": [
        {{
          "date_range": "YYYY-MM-DD ~ YYYY-MM-DD",
          "peak": "YYYY-MM-DD",
          "change": "+10.5%",
          "reason": "ë‰´ìŠ¤ ê¸°ë°˜ ì›ì¸ ë¶„ì„ (2-3ë¬¸ì¥)",
          "news_summary": "ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½ (1ë¬¸ì¥)",
          "sentiment": "ê¸ì •ì /ë¶€ì •ì /ì¤‘ë¦½ì  ë° ì‹œì¥ í‰ê°€"
        }}
      ]
    }}
    """
    try:
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role":"system","content":"You are a financial expert. Analyze stock volatility ONLY based on provided news. Do not speculate. Return ONLY valid JSON."},{"role":"user","content":prompt}],
            temperature=0.1, response_format={"type": "json_object"}
        )
        return json.loads(completion.choices[0].message.content)
    except:
        return {"events": [], "note": "ë¶„ì„ ì‹¤íŒ¨"}

# --------------------------------------------------------------------------
# 4. ë©”ì¸ UI
# --------------------------------------------------------------------------

krx_map = get_krx_code_map()

with st.sidebar:
    st.header("âš¡ AlphaView: Deep Dive")
    name_input = st.text_input("ì¢…ëª©ëª…", "ì¹´ì¹´ì˜¤")
    ticker = find_ticker(name_input, krx_map)
    st.divider()
    s_date = st.date_input("ì‹œì‘ì¼", date(2024, 11, 19))
    e_date = st.date_input("ì¢…ë£Œì¼", date(2025, 11, 19))
    run_btn = st.button("ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ ğŸš€", type="primary")

st.title(f"AlphaView: {name_input} ({ticker})")

if run_btn:
    with st.status("ë°ì´í„° ìˆ˜ì§‘ ë° ì •ë°€ ë¶„ì„ ì¤‘...", expanded=True) as status:
        st.write("1. ì£¼ê°€ & íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘...")
        df_stock = get_stock_data(ticker, s_date, e_date)
        df_trend = get_naver_datalab_trend(name_input, s_date, e_date)
        
        st.write("2. ë‰´ìŠ¤ ì•„ì¹´ì´ë¸Œ ê²€ìƒ‰...")
        news_items = get_naver_news_content(name_input, s_date, e_date)
        
        # ë°ì´í„° ë³‘í•© ë° ë³€ë™ì„± ê³„ì‚°
        top_events_df = pd.DataFrame()
        merged_df = pd.DataFrame()
        if df_stock is not None and not df_trend.empty:
            aligned_trend = df_trend.reindex(df_stock.index).fillna(0)
            df_stock['DailyReturn'] = df_stock['Close'].pct_change() * 100
            df_stock['AbsReturn'] = df_stock['DailyReturn'].abs()

            merged_df = pd.DataFrame({
                'Close': df_stock['Close'],
                'DailyReturn': df_stock['DailyReturn'],
                'AbsReturn': df_stock['AbsReturn'],
                'SearchVolume': aligned_trend['search_volume'],
                'Volume': df_stock.get('Volume', pd.Series(index=df_stock.index))
            })

            # ë³´ê°„/ê²°ì¸¡ ì²˜ë¦¬ (í•„ìš”ì‹œ)
            merged_df = merged_df.dropna(subset=['Close'])

            # ë¡¤ë§ ê¸°ë°˜ z-score(ì´ìƒì¹˜ íƒì§€)ë¡œ ì´ë²¤íŠ¸(ê¸°ê°„) ì¶”ì¶œ
            # ë°ì´í„° í¬ê¸°ì— ë§ì¶° rolling window ë™ì  ì¡°ì •
            n_data = len(merged_df)
            if n_data < 30:
                window_size = max(5, n_data // 6)
                min_periods = max(3, window_size // 2)
            else:
                window_size = 30
                min_periods = 10
            
            roll = merged_df[['DailyReturn', 'SearchVolume', 'Volume']].rolling(window=window_size, min_periods=min_periods)
            mean = roll.mean()
            std = roll.std().replace(0, np.nan)

            merged_df['ret_z'] = (merged_df['DailyReturn'] - mean['DailyReturn']) / std['DailyReturn']
            merged_df['search_z'] = (merged_df['SearchVolume'] - mean['SearchVolume']) / std['SearchVolume']
            merged_df['vol_z'] = (merged_df['Volume'] - mean['Volume']) / std['Volume']

            # ì´ë²¤íŠ¸ í”Œë˜ê·¸: ìˆ˜ìµë¥  ë˜ëŠ” ê²€ìƒ‰ëŸ‰ ë˜ëŠ” ê±°ë˜ëŸ‰ì—ì„œ ìœ ì˜í•œ ì´ìƒì¹˜
            # z-score ì„ê³„ê°’ë„ ë°ì´í„°ëŸ‰ì— ë§ì¶° ì¡°ì •
            z_threshold = 1.8 if n_data < 100 else 2.0
            merged_df['evt_flag'] = (merged_df['ret_z'].abs() > z_threshold) | (merged_df['search_z'] > z_threshold) | (merged_df['vol_z'] > z_threshold)

            # ì—°ì†ëœ True êµ¬ê°„ì„ ì´ë²¤íŠ¸ë¡œ ë¬¶ê¸°
            groups = []
            in_group = False
            start = None
            end = None
            for idx, row in merged_df.iterrows():
                if row['evt_flag'] and not in_group:
                    in_group = True
                    start = idx
                    end = idx
                elif row['evt_flag'] and in_group:
                    end = idx
                elif (not row['evt_flag']) and in_group:
                    groups.append((start, end))
                    in_group = False
            if in_group and start is not None:
                groups.append((start, end))

            events = []
            for s, e in groups:
                window = merged_df.loc[s:e]
                if window.empty: continue
                
                event_duration = (e.date() - s.date()).days + 1
                if event_duration < 1:
                    continue
                
                # í”¼í¬ ë‚ ì§œ: ì ˆëŒ€ ë³€ë™í­ ìµœëŒ€ê°’ ê¸°ì¤€
                peak_idx = window['AbsReturn'].idxmax()
                peak_change = window.loc[peak_idx, 'DailyReturn'] if not pd.isna(peak_idx) else 0.0
                peak_abs = abs(peak_change)
                
                # í•„í„°: ë³€ë™í­ ìµœì†Œê°’ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì˜ë¯¸ ì—†ìŒ)
                if peak_abs < 1.0:  # ìµœì†Œ 1% ì´ìƒ ë³€ë™
                    continue
                
                # íŠ¸ë¦¬ê±° íŒíŠ¸: ì–´ë–¤ ì§€í‘œê°€ ê°€ì¥ í¬ì—ˆëŠ”ì§€
                trig_vals = {
                    'return': window['ret_z'].abs().max(),
                    'search': window['search_z'].max(),
                    'volume': window['vol_z'].max()
                }
                trigger = max(trig_vals, key=lambda k: -np.nan_to_num(-trig_vals[k]))
                
                # ã€NEWã€‘ ì˜ë¯¸ì„± ì ìˆ˜ ê³„ì‚° (ê¸°ê°„ + ë³€ë™í­ + í‰ê·  ë³€ë™ì„±)
                # ë‹¨ì¼ ì¼ì˜ í° ë³€ë™ vs ì¥ê¸° ì¶”ì„¸ ë³€í™”ë¥¼ ëª¨ë‘ ê³ ë ¤
                avg_abs_return = window['AbsReturn'].mean()
                cumulative_return = window['DailyReturn'].sum()
                
                # ì ìˆ˜ êµ¬ì„±:
                # 1. í”¼í¬ ë‹¨ì¼ ë³€ë™í­ (ë‹¨ê¸° ê¸‰ë“±ë½)
                peak_score = peak_abs
                
                # 2. ê¸°ê°„ í‰ê·  ë³€ë™ì„± (ì¶”ì„¸ì˜ ì¼ê´€ì„±)
                duration_factor = min(event_duration / 30, 1.0)  # ìµœëŒ€ 30ì¼ ê¸°ì¤€
                trend_score = avg_abs_return * duration_factor * 2
                
                # 3. ëˆ„ì  ë³€ë™í­ (ì¥ê¸° ì¶”ì„¸)
                cumulative_score = abs(cumulative_return) * 0.5
                
                # ì¢…í•© ì˜ë¯¸ì„± ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
                significance_score = (peak_score * 0.5 + trend_score * 0.3 + cumulative_score * 0.2)
                
                # ìˆ˜ì§‘ëœ ë‰´ìŠ¤(êµ¬ê°„ Â±2ì¼ë¡œ í™•ëŒ€, ìš”ì•½ ì œëª© í¬í•¨)
                related_news = []
                for item in news_items:
                    try:
                        nd = item['clean_date'].date()
                        if (nd >= s.date() - timedelta(days=2)) and (nd <= e.date() + timedelta(days=2)):
                            related_news.append({
                                'date': item.get('date_str'), 
                                'title': clean_text(item.get('title')),
                                'summary_title': item.get('summary_title', clean_text(item.get('title'))),  # ìš”ì•½ ì œëª©
                                'link': item.get('link')
                            })
                    except:
                        continue

                events.append({
                    'Start': s,
                    'End': e,
                    'PeakDate': peak_idx,
                    'PeakChange': peak_change,
                    'PeakChangeAbs': peak_abs,
                    'Duration': event_duration,
                    'AvgReturn': avg_abs_return,
                    'CumulativeReturn': cumulative_return,
                    'SignificanceScore': significance_score,
                    'Trigger': trigger,
                    'NewsList': related_news
                })

            if events:
                top_events_df = pd.DataFrame(events).sort_values('SignificanceScore', ascending=False)
                
                # ë‰´ìŠ¤ ë³´ìœ  ì—¬ë¶€ë¡œ í•„í„°ë§: ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‰´ìŠ¤ë¥¼ ê°€ì§„ ì´ë²¤íŠ¸ë§Œ ì„ íƒ
                events_with_news = top_events_df[top_events_df['NewsList'].apply(len) >= 2]
                
                if not events_with_news.empty:
                    # ë‰´ìŠ¤ ì¶©ë¶„í•œ ì´ë²¤íŠ¸ ì¤‘ ìƒìœ„ 3ê°œ ì„ íƒ (ì˜ë¯¸ì„± ì ìˆ˜ìˆœ)
                    top_events_df = events_with_news.head(3)
                    st.write(f"âœ… {len(events)}ê°œì˜ ì´ë²¤íŠ¸ íƒì§€, ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„ ê°€ëŠ¥í•œ TOP3 ì„ ë³„ (ì˜ë¯¸ì„± ì ìˆ˜ìˆœ)")
                else:
                    # ë‰´ìŠ¤ ë¶€ì¡± ì‹œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ì˜ë¯¸ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ë˜, ë¶„ì„ ì‹œ ê²½ê³ 
                    top_events_df = top_events_df.head(3)
                    st.warning(f"âš ï¸ {len(events)}ê°œ ì´ë²¤íŠ¸ íƒì§€ë˜ì—ˆìœ¼ë‚˜, ê´€ë ¨ ë‰´ìŠ¤ ë¶€ì¡±ìœ¼ë¡œ ì‹œì¥ ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì…ë‹ˆë‹¤.")

        st.write("3. AI ì¢…í•© ë¶„ì„ (íŠ¸ë Œë“œ + ê¸‰ë“±ë½ ì›ì¸)...")
        ai_general = analyze_general_trend(name_input, df_trend, news_items)
        ai_volatility = analyze_top_volatility(name_input, top_events_df if not top_events_df.empty else pd.DataFrame(), news_items)
        
        if top_events_df.empty:
            st.warning("âš ï¸ íƒì§€ëœ ê¸‰ë“±ë½ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì„ ë³€ê²½í•´ë³´ì„¸ìš”.")

        status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

    # ------------------------------------------------
    # [ì‹œê°í™”] ì‹œê³„ì—´ ì°¨íŠ¸ (ì„  + ì„ )
    # ------------------------------------------------
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Š ì£¼ê°€ vs ëŒ€ì¤‘ ê´€ì‹¬ë„")
        
        if not merged_df.empty:
            fig = make_subplots(specs=[[{"secondary_y": True}]])
            
            # 1. ê²€ìƒ‰ëŸ‰ (ì–‡ì€ ì„ , ë°°ê²½ì— ê¹”ë¦¼ â€” ê°€ê²©ì„ ê°€ë¦¬ì§€ ì•Šë„ë¡ ì•„ë˜ì— ê·¸ë¦¼)
            fig.add_trace(
                go.Scatter(
                    x=merged_df.index,
                    y=merged_df['SearchVolume'],
                    name="ê²€ìƒ‰ëŸ‰(Trend)",
                    mode='lines',
                    line=dict(color='#ff7f0e', width=1.6, dash='dot'),
                    opacity=0.55,
                ),
                secondary_y=True
            )

            # 2. ì£¼ê°€ (ì§„í•œ ì„ , ìœ„ì— ê·¸ë¦¼)
            fig.add_trace(
                go.Scatter(x=merged_df.index, y=merged_df['Close'], name="ì£¼ê°€",
                           line=dict(color='#1f77b4', width=3)),
                secondary_y=False
            )
            
            # Top 3 ì´ë²¤íŠ¸ë§Œ ì‹œê°ì ìœ¼ë¡œ êµ¬ê°„ í‘œì‹œ (vrect + ë²ˆí˜¸ ë¼ë²¨)
            if not top_events_df.empty:
                top3 = top_events_df.head(3)
                colors = ['rgba(255, 0, 0, 0.15)', 'rgba(255, 165, 0, 0.15)', 'rgba(0, 0, 255, 0.15)']  # ë¹¨ê°•, ì£¼í™©, íŒŒë‘
                for rank, (_, ev) in enumerate(top3.iterrows()):
                    try:
                        # êµ¬ê°„ í‘œì‹œ (vrect)
                        fig.add_vrect(
                            x0=ev['Start'], x1=ev['End'],
                            fillcolor=colors[rank], opacity=1.0,
                            layer='below', line_width=2,
                            annotation_text=f"#{rank+1}",
                            annotation_position='top left',
                            annotation_font=dict(size=14, color='black')
                        )
                    except:
                        continue

            fig.update_yaxes(title_text="ì£¼ê°€", secondary_y=False)
            fig.update_yaxes(title_text="íŠ¸ë Œë“œ ì§€ìˆ˜", secondary_y=True, showgrid=False) # ì˜¤ë¥¸ìª½ ê·¸ë¦¬ë“œ ì œê±°
            fig.update_layout(hovermode="x unified", height=450)
            
            st.plotly_chart(fig, use_container_width=True)
            st.caption("â€» ì£¼í™©ìƒ‰ ì ì„ ì€ ê²€ìƒ‰ ê´€ì‹¬ë„ì…ë‹ˆë‹¤. ìƒ‰ìƒ êµ¬ê°„(#1,#2,#3)ì€ ìƒìœ„ 3ëŒ€ ê¸‰ë“±ë½ ê¸°ê°„ì…ë‹ˆë‹¤.")

    # ------------------------------------------------
    # [AI ë¦¬í¬íŠ¸] ì¼ë°˜ ìš”ì•½ + ê¸‰ë“±ë½ ì›ì¸
    # ------------------------------------------------
    with col2:
        st.subheader("ğŸ¤– AI ì‹¬ì¸µ ë¦¬í¬íŠ¸")
        
        # 1. ì¼ë°˜ ìš”ì•½
        if ai_general:
            with st.expander("ğŸ“Œ ì „ì²´ íë¦„ ìš”ì•½", expanded=True):
                st.info(ai_general.get('summary'))
                st.write(f"**ì‹¬ë¦¬:** {ai_general.get('sentiment')}")
        
        # 2. ê¸‰ë“±ë½ ì›ì¸ ë¶„ì„ (Top 3 ë‰´ìŠ¤ ê¸°ë°˜) + ê´€ë ¨ ë‰´ìŠ¤ + ì´ë²¤íŠ¸ íŠ¹ì„±
        st.markdown("#### ğŸš¨ ê¸‰ë“±ë½ ì›ì¸ ë¶„ì„ (ë‰´ìŠ¤ ê¸°ë°˜ TOP3)")
        if ai_volatility and isinstance(ai_volatility, dict):
            # ë‰´ìŠ¤ ë¶€ì¡± ê²½ê³ 
            if ai_volatility.get('note'):
                st.warning(f"âš ï¸ {ai_volatility.get('note')} - ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.")
            
            events = ai_volatility.get('events', [])
            if events:
                for i, event in enumerate(events[:3]):
                    date_range = event.get('date_range') or event.get('date') or event.get('peak') or 'N/A'
                    change = event.get('change') or ''
                    reason = event.get('reason') or event.get('reasoning') or event.get('detail') or ''
                    news_summary = event.get('news_summary', '')
                    sentiment = event.get('sentiment', '')

                    icon = ""
                    try:
                        if isinstance(change, str) and "+" in change:
                            icon = "ğŸ“ˆ"
                        elif isinstance(change, str) and "-" in change:
                            icon = "ğŸ“‰"
                        else:
                            chv = float(str(change).replace('%', ''))
                            icon = "ğŸ“ˆ" if chv > 0 else "ğŸ“‰"
                    except:
                        icon = ""

                    with st.container(border=True):
                        st.markdown(f"**#{i+1}. {date_range} {icon} {change}**")
                        
                        # ì´ë²¤íŠ¸ íŠ¹ì„± ì •ë³´ (ê¸°ê°„, ì ìˆ˜ ë“±)
                        if not top_events_df.empty and i < len(top_events_df):
                            ev = top_events_df.iloc[i]
                            duration = ev.get('Duration', 0)
                            significance = ev.get('SignificanceScore', 0)
                            avg_return = ev.get('AvgReturn', 0)
                            cumulative = ev.get('CumulativeReturn', 0)
                            
                            # ì´ë²¤íŠ¸ íƒ€ì… íŒë‹¨
                            if duration <= 3:
                                event_type = "ğŸ”´ ë‹¨ê¸° ê¸‰ë“±ë½"
                            elif duration <= 14:
                                event_type = "ğŸŸ  ì¤‘ê¸° ì¶”ì„¸ ë³€í™”"
                            else:
                                event_type = "ğŸŸ¡ ì¥ê¸° ì¶”ì„¸ ë³€í™”"
                            
                            st.caption(f"{event_type} | ê¸°ê°„: {duration}ì¼ | ì¼í‰ê·  ë³€ë™: {avg_return:.2f}% | ëˆ„ì : {cumulative:.2f}%")
                        
                        # ë‰´ìŠ¤ ìš”ì•½ (AIê°€ ì œê³µí•œ í•µì‹¬ ìš”ì•½)
                        if news_summary:
                            st.info(f"ğŸ“° í•µì‹¬ ë‰´ìŠ¤: {news_summary}")
                        
                        # ì›ì¸ ë¶„ì„
                        if reason:
                            st.write(f"**ë¶„ì„:** {reason}")
                        
                        # ì‹œì¥ í‰ê°€ ë° ê°ì •
                        if sentiment:
                            st.write(f"**í‰ê°€:** {sentiment}")
                        
                        # ì°¸ê³ : ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ë“¤ (ì§§ì€ ë²„ì „)
                        if not top_events_df.empty and i < len(top_events_df):
                            ev = top_events_df.iloc[i]
                            if ev.get('NewsList') and len(ev.get('NewsList', [])) > 0:
                                st.caption("ğŸ“‘ ê´€ë ¨ ë‰´ìŠ¤ ì œëª©:")
                                for n in ev['NewsList'][:3]:
                                    # ìš”ì•½ëœ ì œëª© ìš°ì„  í‘œì‹œ
                                    display_title = n.get('summary_title') or n.get('title')
                                    st.caption(f"â€¢ {display_title}")
            else:
                st.info("ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„ì´ ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê¸°ê°„ì„ ì‹œë„í•´ë³´ì„¸ìš”.")

    # ------------------------------------------------
    # [ìƒê´€ê´€ê³„] íƒ­ìœ¼ë¡œ ë¶„ë¦¬
    # ------------------------------------------------
    st.divider()
    st.subheader("ğŸ”— ìƒê´€ê´€ê³„ ì‹¬í™” ë¶„ì„")
    
    corr_vol = merged_df[['SearchVolume', 'AbsReturn']].corr().iloc[0, 1]
    
    c1, c2 = st.columns(2)
    with c1:
        fig_vol = px.scatter(
            merged_df, x='SearchVolume', y='AbsReturn',
            title=f"ê²€ìƒ‰ëŸ‰ vs ì£¼ê°€ ë³€ë™í­ (R={corr_vol:.2f})",
            labels={'SearchVolume': 'ê²€ìƒ‰ëŸ‰ (0-100)', 'AbsReturn': 'ë³€ë™í­ (|%|)'},
            trendline='ols', color_discrete_sequence=['#d62728']
        )
        st.plotly_chart(fig_vol, use_container_width=True)
    
    with c2:
        st.markdown("#### ğŸ’¡ í†µê³„ì  í•´ì„")
        st.write(f"í˜„ì¬ ìƒê´€ê³„ìˆ˜ **R = {corr_vol:.2f}** ì…ë‹ˆë‹¤.")
        if corr_vol > 0.3:
            st.success("ëŒ€ì¤‘ì˜ ê´€ì‹¬ì´ ë†’ì•„ì§ˆìˆ˜ë¡ **ì£¼ê°€ê°€ í¬ê²Œ ìš”ë™ì¹˜ëŠ”(ë³€ë™ì„± í™•ëŒ€)** ê²½í–¥ì´ ëšœë ·í•©ë‹ˆë‹¤.")
        elif corr_vol > 0.1:
            st.info("ê´€ì‹¬ì´ ë†’ì•„ì§€ë©´ ë³€ë™ì„±ì´ **ì•½ê°„ ì»¤ì§€ëŠ”** ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ê²€ìƒ‰ëŸ‰ê³¼ ë³€ë™ì„± ì‚¬ì´ì— ëšœë ·í•œ ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ë‰´ìŠ¤ íƒ€ì„ë¼ì¸
    if news_items:
        with st.expander("ğŸ—ï¸ ì „ì²´ ë‰´ìŠ¤ íƒ€ì„ë¼ì¸ ë³´ê¸°"):
            for item in news_items:
                st.write(f"**{item['date_str']}** | [{clean_text(item['title'])}]({item['link']})")




def plot_correlation_heatmap(portfolio_data):
    """ì¢…ëª©ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"""
    try:
        if not portfolio_data:
            return None
        
        if "stock_data" not in portfolio_data:
            return None
        
        stock_data_dict = portfolio_data.get("stock_data", {})
        
        if not stock_data_dict or len(stock_data_dict) < 2:
            return None
        
        # ê° ì¢…ëª©ì˜ ì¢…ê°€ ë°ì´í„° ì¶”ì¶œ
        price_data = {}
        for ticker, data in stock_data_dict.items():
            if isinstance(data, pd.DataFrame) and not data.empty:
                if 'Close' in data.columns:
                    price_data[ticker] = data['Close']
        
        if len(price_data) < 2:
            return None
        
        # DataFrameìœ¼ë¡œ í†µí•©
        prices_df = pd.DataFrame(price_data)
        prices_df = prices_df.dropna()
        
        if prices_df.empty or prices_df.shape[0] < 2:
            return None
        
        # ìˆ˜ìµë¥ ë¡œ ë³€í™˜
        returns_df = prices_df.pct_change().dropna()
        
        if returns_df.empty or returns_df.shape[0] < 2 or returns_df.shape[1] < 2:
            return None
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        try:
            corr_matrix = returns_df.corr()
        except:
            return None
        
        if corr_matrix is None or corr_matrix.empty:
            return None
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig = go.Figure(data=go.Heatmap(
            z=corr_matrix.values,
            x=list(corr_matrix.columns),
            y=list(corr_matrix.columns),
            colorscale='RdBu',
            zmid=0,
            text=np.round(corr_matrix.values, 2),
            texttemplate='%{text}',
            textfont={"size": 11},
            colorbar=dict(title="ìƒê´€ê³„ìˆ˜"),
            zmin=-1,
            zmax=1
        ))
        
        fig.update_layout(
            title='ğŸ“Š ì¢…ëª©ê°„ ìƒê´€ê´€ê³„ (ë¶„ì‚° íš¨ê³¼ ë¶„ì„)',
            height=450,
            xaxis_title='ì¢…ëª©',
            yaxis_title='ì¢…ëª©',
            template='plotly_white',
            hovermode='closest'
        )
        
        return fig
    except Exception as e:
        return None
